import os
import torch
import requests
from shutil import rmtree
from zipfile import ZipFile
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BertTokenizer, BertModel
from dotenv import load_dotenv

load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π
MODEL_DIR = os.getenv("MODEL_PATH", "/content/rut5_simplifier_new")
ZIP_PATH = "/content/rut5_simplifier.zip"
PUBLIC_LINK = "https://disk.yandex.ru/d/GcR3ougL6bY6kw"

# –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞
def download_from_yandex_disk(public_url, output_path):
    base_url = "https://cloud-api.yandex.net/v1/disk/public/resources/download"
    params = {"public_key": public_url}
    response = requests.get(base_url, params=params)
    response.raise_for_status()
    download_url = response.json()["href"]
    print("üì• –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å —Å –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞...")
    session = requests.Session()
    session.mount('https://', requests.adapters.HTTPAdapter(pool_connections=1, pool_maxsize=10))
    with session.get(download_url, stream=True) as r:
        r.raise_for_status()
        with open(output_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
    print(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")

# –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏
def extract_model(zip_path, extract_to):
    print("üì¶ –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∞—Ä—Ö–∏–≤...")
    with ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–∞ –≤: {extract_to}")

# –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π
def load_model(model_path, model_type):
    try:
        print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º {model_type} –º–æ–¥–µ–ª—å –∏–∑: {model_path}")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è GPU
        if device == "cuda":
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if gpu_memory < 8:
                model.half()
                print("üîß –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–∞ –≤ float16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏")
        print(f"‚úÖ {model_type} –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        return tokenizer, model
    except Exception as e:
        error_msg = f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {model_type} –º–æ–¥–µ–ª–∏: {e}"
        print(error_msg)
        raise RuntimeError(error_msg)

async def download_and_setup_models():
    print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
    if not os.path.exists(MODEL_DIR):
        print("üîç –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ. –ó–∞–≥—Ä—É–∂–∞–µ–º...")
        try:
            download_from_yandex_disk(PUBLIC_LINK, ZIP_PATH)
            extract_model(ZIP_PATH, MODEL_DIR)
            os.remove(ZIP_PATH)
        except Exception as e:
            if os.path.exists(ZIP_PATH):
                os.remove(ZIP_PATH)
            if os.path.exists(MODEL_DIR):
                rmtree(MODEL_DIR)
            raise RuntimeError(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    else:
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {MODEL_DIR}")
    
    # –Ø–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –ø—É—Ç–∏ –∫ –º–æ–¥–µ–ª–∏
    MODEL_SUBDIR = "rut5_simplifier"
    MODEL_PATH = os.path.join(MODEL_DIR, MODEL_SUBDIR)
    if not os.path.exists(MODEL_PATH):
        raise RuntimeError(f"‚ùå –ü–∞–ø–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {MODEL_PATH}")
    print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {MODEL_PATH}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏
    required_files = ['config.json']
    model_files = ['pytorch_model.bin', 'model.safetensors']
    missing_files = []
    
    for file in required_files:
        if not os.path.exists(os.path.join(MODEL_PATH, file)):
            missing_files.append(file)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏
    model_found = False
    for model_file in model_files:
        file_path = os.path.join(MODEL_PATH, model_file)
        if os.path.exists(file_path):
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏: {model_file}")
            model_found = True
            break
    
    if not model_found:
        missing_files.extend(model_files)
    
    if missing_files:
        raise RuntimeError(f"‚ùå –í –ø–∞–ø–∫–µ –º–æ–¥–µ–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã: {missing_files}")
    print("‚úÖ –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
    try:
        simplify_tokenizer, simplify_model = load_model(MODEL_PATH, "—É–ø—Ä–æ—â–µ–Ω–∏—è")
    except Exception as e:
        raise RuntimeError(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ —É–ø—Ä–æ—â–µ–Ω–∏—è: {e}")
    
    TRANSLATE_MODEL_NAME = "Helsinki-NLP/opus-mt-ru-en"
    try:
        translator_tokenizer, translator_model = load_model(TRANSLATE_MODEL_NAME, "–ø–µ—Ä–µ–≤–æ–¥–∞")
    except Exception as e:
        raise RuntimeError(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BERT
    bert_model_name = "bert-base-multilingual-cased"
    bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)
    bert_model = BertModel.from_pretrained(bert_model_name)
    bert_model.to(device)
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è Colab
    torch.backends.cuda.max_split_size_mb = 512
    if device == "cuda":
        torch.cuda.empty_cache()
        print(f"üßπ –û—á–∏—â–µ–Ω –∫—ç—à CUDA. –°–≤–æ–±–æ–¥–Ω–æ –ø–∞–º—è—Ç–∏: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    print("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    
    return {
        'simplify_tokenizer': simplify_tokenizer,
        'simplify_model': simplify_model,
        'translator_tokenizer': translator_tokenizer,
        'translator_model': translator_model,
        'bert_tokenizer': bert_tokenizer,
        'bert_model': bert_model,
        'device': device
    }