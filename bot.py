# bot.py
# –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª –∑–∞–ø—É—Å–∫–∞ TextEaseBot

import asyncio
import logging
import os
from getpass import getpass
from telegram.ext import Application
from dotenv import load_dotenv

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è ---
load_dotenv()

TOKEN = os.getenv("BOT_TOKEN")
MODEL_PATH = os.getenv("MODEL_PATH")

if not TOKEN:
    print("üîê –¢–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env. –í–≤–µ–¥–∏—Ç–µ –≤—Ä—É—á–Ω—É—é:")
    TOKEN = getpass()
    with open(".env", "a") as f:
        f.write(f"BOT_TOKEN={TOKEN}\n")

if not MODEL_PATH:
    raise ValueError("‚ùå MODEL_PATH –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env. –ó–∞–ø—É—Å—Ç–∏—Ç–µ download_model.py")

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π ---
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —É–ø—Ä–æ—â–µ–Ω–∏—è...")
simplify_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
simplify_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device)

print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞...")
translator_tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-ru-en")
translator_model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-ru-en").to(device)

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Ç–∏–ª–∏—Ç ---
import utils
import handlers
import callbacks
from nltk.tokenize import sent_tokenize

# –ü–µ—Ä–µ–¥–∞—ë–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ –º–æ–¥—É–ª–∏
utils.simplify_tokenizer = simplify_tokenizer
utils.simplify_model = simplify_model
utils.translator_tokenizer = translator_tokenizer
utils.translator_model = translator_model
utils.device = device
utils.sent_tokenize = sent_tokenize

handlers.sent_tokenize = sent_tokenize

callbacks.simplify_long_text = utils.simplify_long_text
callbacks.translate_text = utils.translate_text
callbacks.evaluate_simplification = utils.evaluate_simplification
callbacks.split_text = utils.split_text
callbacks.sent_tokenize = sent_tokenize
callbacks.logger = logger

# --- –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ ---
async def main():
    print("üöÄ –°–æ–∑–¥–∞—ë–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ...")
    app = Application.builder().token(TOKEN).build()

    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
    handlers.register_handlers(app)
    callbacks.register_callbacks(app)

    print("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è...")
    while True:
        try:
            await app.run_polling(
                poll_interval=2,
                timeout=20,
                drop_pending_updates=True
            )
        except Exception as e:
            logger.warning(f"üîÅ –û—à–∏–±–∫–∞: {e}. –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ 5 —Å–µ–∫...")
            await asyncio.sleep(5)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("üõë –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤—Ä—É—á–Ω—É—é.")
        