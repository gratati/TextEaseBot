{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNe1YhaYPhWQ70PMIboQwPI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gratati/TextEaseBot/blob/main/%22TextEaseBot_copy_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π ---\n",
        "!pip install -q python-telegram-bot==20.6 transformers torch python-docx nltk nest-asyncio python-dotenv sacrebleu sentencepiece langdetect chardet spacy\n",
        "\n",
        "# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ spacy ---\n",
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "id": "yuKSZNFazNcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecaf42b1-772f-439d-bc3d-57c481924296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pymorphy3>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ru-core-news-sm==3.8.0) (2.0.4)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (2.4.417150.4580142)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (75.2.0)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- –ò–º–ø–æ—Ä—Ç—ã –∏ NLTK ---\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import signal\n",
        "import sys\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackQueryHandler\n",
        "\n",
        "import os\n",
        "import re\n",
        "import logging\n",
        "import torch\n",
        "import nltk\n",
        "import sys\n",
        "sys.setrecursionlimit(10000)\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "from shutil import rmtree\n",
        "import shutil\n",
        "import tempfile\n",
        "import random\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup\n",
        "from telegram.ext import (\n",
        "    Application, CommandHandler, MessageHandler,\n",
        "    CallbackQueryHandler, ContextTypes, filters\n",
        ")\n",
        "from docx import Document\n",
        "from langdetect import detect, LangDetectException\n",
        "import chardet\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ NLTK ---\n",
        "nltk_data_dir = \"/content/nltk_data\"\n",
        "os.makedirs(nltk_data_dir, exist_ok=True)\n",
        "nltk.data.path.insert(0, nltk_data_dir)\n",
        "\n",
        "required_nltk_packages = [\n",
        "    ('punkt', 'tokenizers/punkt'),\n",
        "    ('punkt_tab', 'tokenizers/punkt_tab'),\n",
        "    ('stopwords', 'corpora/stopwords')\n",
        "]\n",
        "\n",
        "for package, path in required_nltk_packages:\n",
        "    try:\n",
        "        nltk.data.find(path)\n",
        "        print(f\"‚úÖ {package} —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
        "    except LookupError:\n",
        "        print(f\"üì• –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º {package}...\")\n",
        "        nltk.download(package, download_dir=nltk_data_dir, quiet=False)\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# --- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ---\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    level=logging.INFO\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
        "\n",
        "# --- –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è ---\n",
        "load_dotenv()\n",
        "\n",
        "# –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n",
        "BOT_TOKEN = None\n",
        "\n",
        "# –°–ø–æ—Å–æ–± 1: –ò–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è (.env —Ñ–∞–π–ª)\n",
        "if not BOT_TOKEN:\n",
        "    BOT_TOKEN = os.getenv(\"BOT_TOKEN\")\n",
        "\n",
        "# –°–ø–æ—Å–æ–± 2: –ò–∑ Colab User Data (–±–µ–∑–æ–ø–∞—Å–Ω–æ)\n",
        "if not BOT_TOKEN or BOT_TOKEN == \"YOUR_BOT_TOKEN_HERE\":\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        BOT_TOKEN = userdata.get('BOT_TOKEN')\n",
        "        if BOT_TOKEN:\n",
        "            print(\"‚úÖ –¢–æ–∫–µ–Ω –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ Colab User Data\")\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∞\n",
        "if not BOT_TOKEN or BOT_TOKEN == \"YOUR_BOT_TOKEN_HERE\":\n",
        "    raise RuntimeError(\"‚ùå –¢–æ–∫–µ–Ω –±–æ—Ç–∞ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω. –ë–æ—Ç –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–ø—É—â–µ–Ω.\")\n",
        "\n",
        "print(f\"‚úÖ –¢–æ–∫–µ–Ω –±–æ—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω: {BOT_TOKEN[:10]}...\")\n",
        "\n",
        "# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞ ---\n",
        "def download_from_yandex_disk(public_url, output_path):\n",
        "    base_url = \"https://cloud-api.yandex.net/v1/disk/public/resources/download\"\n",
        "    params = {\"public_key\": public_url}\n",
        "    response = requests.get(base_url, params=params)\n",
        "    response.raise_for_status()\n",
        "    download_url = response.json()[\"href\"]\n",
        "\n",
        "    print(\"üì• –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å —Å –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞...\")\n",
        "    session = requests.Session()\n",
        "    session.mount('https://', requests.adapters.HTTPAdapter(pool_connections=1, pool_maxsize=10))\n",
        "\n",
        "    with session.get(download_url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "    print(f\"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}\")\n",
        "\n",
        "def extract_model(zip_path, extract_to):\n",
        "    print(\"üì¶ –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∞—Ä—Ö–∏–≤...\")\n",
        "    with ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(f\"‚úÖ –ú–æ–¥–µ–ª—å —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–∞ –≤: {extract_to}\")\n",
        "\n",
        "# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π ---\n",
        "MODEL_DIR = \"/content/rut5_simplifier_new\"\n",
        "ZIP_PATH = \"/content/rut5_simplifier.zip\"\n",
        "PUBLIC_LINK = \"https://disk.yandex.ru/d/GcR3ougL6bY6kw\"\n",
        "\n",
        "# --- –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ ---\n",
        "print(\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...\")\n",
        "\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    print(\"üîç –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ. –ó–∞–≥—Ä—É–∂–∞–µ–º...\")\n",
        "    try:\n",
        "        download_from_yandex_disk(PUBLIC_LINK, ZIP_PATH)\n",
        "        extract_model(ZIP_PATH, MODEL_DIR)\n",
        "        os.remove(ZIP_PATH)\n",
        "    except Exception as e:\n",
        "        if os.path.exists(ZIP_PATH):\n",
        "            os.remove(ZIP_PATH)\n",
        "        if os.path.exists(MODEL_DIR):\n",
        "            rmtree(MODEL_DIR)\n",
        "        raise RuntimeError(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}\")\n",
        "else:\n",
        "    print(f\"‚úÖ –ú–æ–¥–µ–ª—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {MODEL_DIR}\")\n",
        "\n",
        "# --- –Ø–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –ø—É—Ç–∏ –∫ –º–æ–¥–µ–ª–∏ ---\n",
        "MODEL_SUBDIR = \"rut5_simplifier\"\n",
        "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_SUBDIR)\n",
        "\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    raise RuntimeError(f\"‚ùå –ü–∞–ø–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {MODEL_PATH}\")\n",
        "\n",
        "print(f\"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {MODEL_PATH}\")\n",
        "\n",
        "# --- –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏ ---\n",
        "required_files = ['config.json']\n",
        "model_files = ['pytorch_model.bin', 'model.safetensors']\n",
        "\n",
        "missing_files = []\n",
        "for file in required_files:\n",
        "    if not os.path.exists(os.path.join(MODEL_PATH, file)):\n",
        "        missing_files.append(file)\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏\n",
        "model_found = False\n",
        "for model_file in model_files:\n",
        "    file_path = os.path.join(MODEL_PATH, model_file)\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏: {model_file}\")\n",
        "        model_found = True\n",
        "        break\n",
        "\n",
        "if not model_found:\n",
        "    missing_files.extend(model_files)\n",
        "\n",
        "if missing_files:\n",
        "    raise RuntimeError(f\"‚ùå –í –ø–∞–ø–∫–µ –º–æ–¥–µ–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã: {missing_files}\")\n",
        "\n",
        "print(\"‚úÖ –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç\")\n",
        "\n",
        "# --- –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π ---\n",
        "def load_model(model_path, model_type):\n",
        "    try:\n",
        "        print(f\"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º {model_type} –º–æ–¥–µ–ª—å –∏–∑: {model_path}\")\n",
        "\n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ safetensors\n",
        "        safetensors_path = os.path.join(model_path, \"model.safetensors\")\n",
        "        if os.path.exists(safetensors_path):\n",
        "            print(\"üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç safetensors (–±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏ –±—ã—Å—Ç—Ä—ã–π)\")\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
        "\n",
        "        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è GPU\n",
        "        if device == \"cuda\":\n",
        "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "            if gpu_memory < 8:\n",
        "                model.half()\n",
        "                print(\"üîß –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–∞ –≤ float16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\")\n",
        "\n",
        "        print(f\"‚úÖ {model_type} –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        error_msg = f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {model_type} –º–æ–¥–µ–ª–∏: {e}\"\n",
        "        print(error_msg)\n",
        "        raise RuntimeError(error_msg)\n",
        "\n",
        "# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π ---\n",
        "try:\n",
        "    simplify_tokenizer, simplify_model = load_model(MODEL_PATH, \"—É–ø—Ä–æ—â–µ–Ω–∏—è\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ —É–ø—Ä–æ—â–µ–Ω–∏—è: {e}\")\n",
        "\n",
        "TRANSLATE_MODEL_NAME = \"Helsinki-NLP/opus-mt-ru-en\"\n",
        "try:\n",
        "    translator_tokenizer, translator_model = load_model(TRANSLATE_MODEL_NAME, \"–ø–µ—Ä–µ–≤–æ–¥–∞\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}\")\n",
        "\n",
        "print(\"‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
        "\n",
        "# --- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è Colab ---\n",
        "torch.backends.cuda.max_split_size_mb = 512\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"üßπ –û—á–∏—â–µ–Ω –∫—ç—à CUDA. –°–≤–æ–±–æ–¥–Ω–æ –ø–∞–º—è—Ç–∏: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqKoOygO_KUG",
        "outputId": "f477e343-84a5-4bc6-fed4-80e50a9f70bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ punkt —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\n",
            "‚úÖ punkt_tab —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\n",
            "‚úÖ stopwords —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\n",
            "‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cpu\n",
            "‚úÖ –¢–æ–∫–µ–Ω –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ Colab User Data\n",
            "‚úÖ –¢–æ–∫–µ–Ω –±–æ—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω: 8069120254...\n",
            "üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...\n",
            "‚úÖ –ú–æ–¥–µ–ª—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: /content/rut5_simplifier_new\n",
            "‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: /content/rut5_simplifier_new/rut5_simplifier\n",
            "‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏: model.safetensors\n",
            "‚úÖ –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç\n",
            "üì• –ó–∞–≥—Ä—É–∂–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–∏—è –º–æ–¥–µ–ª—å –∏–∑: /content/rut5_simplifier_new/rut5_simplifier\n",
            "üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç safetensors (–±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏ –±—ã—Å—Ç—Ä—ã–π)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ —É–ø—Ä–æ—â–µ–Ω–∏—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n",
            "üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥–∞ –º–æ–¥–µ–ª—å –∏–∑: Helsinki-NLP/opus-mt-ru-en\n",
            "‚úÖ –ø–µ—Ä–µ–≤–æ–¥–∞ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n",
            "‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BERT\n",
        "bert_model_name = \"bert-base-multilingual-cased\"\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = BertModel.from_pretrained(bert_model_name)\n",
        "bert_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fci0AjgZwvs",
        "outputId": "66abf652-8361-402b-cb2f-814049113135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- –û–±–Ω–æ–≤–∏–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π ---\n",
        "MAX_TEXT_LENGTH = 10000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö\n",
        "MAX_FILE_SIZE = 20 * 1024 * 1024  # 20 –ú–ë\n",
        "MAX_PARTS_FOR_WARNING = 10  # –ï—Å–ª–∏ —á–∞—Å—Ç–µ–π –±–æ–ª—å—à–µ —ç—Ç–æ–≥–æ, –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
        "\n",
        "# --- –§—É–Ω–∫—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º ---\n",
        "def split_text(text, max_chars=2000):\n",
        "    try:\n",
        "        sentences = sent_tokenize(text, language='russian')\n",
        "    except (LookupError, AttributeError) as e:\n",
        "        print(f\"Tokenizer error: {e}\")\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "    parts = []\n",
        "    current = \"\"\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent = sent.strip()\n",
        "        if not sent:\n",
        "            continue\n",
        "\n",
        "        if len(sent) > max_chars:\n",
        "            words = sent.split()\n",
        "            temp = \"\"\n",
        "            for word in words:\n",
        "                if len(temp) + len(word) + 1 <= max_chars:\n",
        "                    temp += f\" {word}\" if temp else word\n",
        "                else:\n",
        "                    if temp:\n",
        "                        parts.append(temp)\n",
        "                    temp = word\n",
        "            if temp:\n",
        "                current = temp\n",
        "                continue\n",
        "\n",
        "        if len(current) + len(sent) + 1 <= max_chars:\n",
        "            current += f\" {sent}\" if current else sent\n",
        "        else:\n",
        "            if current:\n",
        "                parts.append(current)\n",
        "            current = sent\n",
        "\n",
        "    if current:\n",
        "        parts.append(current)\n",
        "\n",
        "    return parts\n",
        "\n",
        "def simplify_text(text, strength=\"medium\", simplify_tokenizer=None, simplify_model=None, device=None):\n",
        "    if not text.strip() or not all([simplify_tokenizer, simplify_model]):\n",
        "        return text\n",
        "\n",
        "    # –£—Å–ø–µ—à–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã\n",
        "    prompts = {\n",
        "        \"strong\": \"–°–¥–µ–ª–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ–π –ø–µ—Ä–µ—Å–∫–∞–∑ –¥–ª—è —à–∫–æ–ª—å–Ω–∏–∫–∞: \",\n",
        "        \"medium\": \"–£–ø—Ä–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –æ—Å–Ω–æ–≤–Ω—É—é –º—ã—Å–ª—å: \"\n",
        "    }\n",
        "\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å\n",
        "    separator = \"|||\"\n",
        "    prompt = prompts.get(strength, prompts[\"medium\"]) + separator + text.strip()\n",
        "\n",
        "    inputs = simplify_tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=1024\n",
        "    ).to(device)\n",
        "\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "    params = {\n",
        "        \"strong\": {\n",
        "            \"max_length\": min(180, input_length + 30),  # –£–≤–µ–ª–∏—á–∏–º –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–º—ã—Å–ª–∞\n",
        "            \"min_length\": max(30, input_length // 2),   # –£–≤–µ–ª–∏—á–∏–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É\n",
        "            \"length_penalty\": 0.8,  # –°–¥–µ–ª–∞–µ–º –º–µ–Ω–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"repetition_penalty\": 1.2\n",
        "        },\n",
        "        \"medium\": {\n",
        "            \"max_length\": min(350, input_length + 40),\n",
        "            \"min_length\": max(50, input_length // 1.5),\n",
        "            \"length_penalty\": 0.9,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"repetition_penalty\": 1.2\n",
        "        }\n",
        "    }\n",
        "\n",
        "    current_params = params[strength]\n",
        "\n",
        "    generation_params = {\n",
        "        \"max_length\": int(current_params[\"max_length\"]),\n",
        "        \"min_length\": int(current_params[\"min_length\"]),\n",
        "        \"num_beams\": 4,\n",
        "        \"do_sample\": True,\n",
        "        \"top_p\": float(current_params[\"top_p\"]),\n",
        "        \"temperature\": float(current_params[\"temperature\"]),\n",
        "        \"length_penalty\": float(current_params[\"length_penalty\"]),\n",
        "        \"repetition_penalty\": float(current_params[\"repetition_penalty\"]),\n",
        "        \"no_repeat_ngram_size\": 3,\n",
        "        \"early_stopping\": True,\n",
        "        \"pad_token_id\": simplify_tokenizer.pad_token_id,\n",
        "        \"eos_token_id\": simplify_tokenizer.eos_token_id\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = simplify_model.generate(\n",
        "            **inputs,\n",
        "            **generation_params\n",
        "        )\n",
        "\n",
        "    result = simplify_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º –≤—Å–µ –¥–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è –∏ —Å–∞–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å\n",
        "    if separator in result:\n",
        "        result = result.split(separator, 1)[1].strip()\n",
        "    else:\n",
        "        patterns_to_remove = [\n",
        "            r\"^.*?–°–¥–µ–ª–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ–π –ø–µ—Ä–µ—Å–∫–∞–∑ –¥–ª—è —à–∫–æ–ª—å–Ω–∏–∫–∞:\\s*\",\n",
        "            r\"^.*?–£–ø—Ä–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –æ—Å–Ω–æ–≤–Ω—É—é –º—ã—Å–ª—å:\\s*\"\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns_to_remove:\n",
        "            result = re.sub(pattern, \"\", result, flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞\n",
        "    result = re.sub(r\"<extra_id_\\d+>\", \"\", result)\n",
        "    result = re.sub(r\"\\s+\", \" \", result).strip()\n",
        "\n",
        "    if not result or len(result) < 10:\n",
        "        return text\n",
        "\n",
        "    return result\n",
        "\n",
        "def simplify_long_text(text, strength=\"medium\", **kwargs):\n",
        "    # –ë–∞–∑–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä —á–∞—Å—Ç–∏\n",
        "    optimal_part_size = 1500\n",
        "\n",
        "    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–µ–ª–∏–∫–æ–º\n",
        "    if len(text) <= optimal_part_size:\n",
        "        return simplify_text(text, strength=strength, **kwargs)\n",
        "\n",
        "    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏\n",
        "    parts = split_text(text, optimal_part_size)\n",
        "\n",
        "    # –ï—Å–ª–∏ —á–∞—Å—Ç–µ–π —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–∏–π —Ä–∞–∑–º–µ—Ä\n",
        "    if len(parts) > 10:\n",
        "        optimal_part_size = 2000\n",
        "        parts = split_text(text, optimal_part_size)\n",
        "\n",
        "    print(f\"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Ä–∞–∑–±–∏—Ç–∞ –Ω–∞ {len(parts)} —á–∞—Å—Ç–µ–π –ø–æ {optimal_part_size} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
        "\n",
        "    simplified_parts = []\n",
        "    for i, part in enumerate(parts):\n",
        "        if part.strip():\n",
        "            print(f\"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ {i+1}/{len(parts)} ({len(part)} —Å–∏–º–≤–æ–ª–æ–≤)...\")\n",
        "            simplified_part = simplify_text(part, strength=strength, **kwargs)\n",
        "            simplified_parts.append(simplified_part)\n",
        "\n",
        "    result = \" \".join(simplified_parts)\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–ª–∏—à–∫–æ–º –ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "    if len(result.split()) < len(text.split()) * 0.5:\n",
        "        print(\"‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π –ø–æ–¥—Ö–æ–¥...\")\n",
        "        parts = split_text(text, 1000)\n",
        "        simplified_parts = []\n",
        "        for i, part in enumerate(parts):\n",
        "            if part.strip():\n",
        "                simplified_part = simplify_text(part, strength=strength, **kwargs)\n",
        "                simplified_parts.append(simplified_part)\n",
        "        result = \" \".join(simplified_parts)\n",
        "\n",
        "    return result\n",
        "\n",
        "def improve_translation_with_bert(text, bert_tokenizer, bert_model, device):\n",
        "    \"\"\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç BERT –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞\"\"\"\n",
        "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç\n",
        "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "\n",
        "    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    sentence_embedding = torch.mean(last_hidden_states, dim=1)\n",
        "\n",
        "    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏\n",
        "    improved_text = text\n",
        "\n",
        "    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏\n",
        "    improved_text = re.sub(r\"\\b(patche)s?\\b\", \"patch\", improved_text, flags=re.IGNORECASE)\n",
        "    improved_text = re.sub(r\"\\banthioxidant\\b\", \"antioxidant\", improved_text, flags=re.IGNORECASE)\n",
        "    improved_text = re.sub(r\"\\bwhitesing\\b\", \"cleansing\", improved_text, flags=re.IGNORECASE)\n",
        "    improved_text = re.sub(r\"\\bhoneycombs\\b\", \"terms\", improved_text, flags=re.IGNORECASE)\n",
        "    improved_text = re.sub(r\"\\bannexing\\b\", \"applying\", improved_text, flags=re.IGNORECASE)\n",
        "    improved_text = re.sub(r\"\\btables\\b\", \"patches\", improved_text, flags=re.IGNORECASE)\n",
        "    improved_text = re.sub(r\"\\banion\\b\", \"negative ion\", improved_text, flags=re.IGNORECASE)\n",
        "    improved_text = re.sub(r\"\\bhave aesthetic design\\b\", \"\", improved_text, flags=re.IGNORECASE)\n",
        "    improved_text = re.sub(r\"\\bcan be used at any time of the day\\b\", \"\", improved_text, flags=re.IGNORECASE)\n",
        "    improved_text = re.sub(r\"\\bwithout interfering with a person's daily life\\b\", \"\", improved_text, flags=re.IGNORECASE)\n",
        "    improved_text = re.sub(r\"\\bit is very convenient to wear applicators\\b\", \"\", improved_text, flags=re.IGNORECASE)\n",
        "    improved_text = re.sub(r\"\\bThis process, in turn, increases the recovery and regeneration capacity\\b\", \"\", improved_text, flags=re.IGNORECASE)\n",
        "    improved_text = re.sub(r\"\\breinforces human immunity\\b\", \"\", improved_text, flags=re.IGNORECASE)\n",
        "    improved_text = re.sub(r\"\\band the allocation of negative ions of anion\\b\", \"\", improved_text, flags=re.IGNORECASE)\n",
        "    improved_text = re.sub(r\"\\bThey have\\b\", \"\", improved_text, flags=re.IGNORECASE)\n",
        "\n",
        "    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –∑–∞–ø—è—Ç—ã–µ\n",
        "    improved_text = re.sub(r\"\\s+\", \" \", improved_text).strip()\n",
        "    improved_text = re.sub(r\"\\s+,\", \",\", improved_text)\n",
        "    improved_text = re.sub(r\",\\s*,\", \",\", improved_text)\n",
        "\n",
        "    # –£–±–∏—Ä–∞–µ–º –∑–∞–ø—è—Ç—ã–µ –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
        "    improved_text = re.sub(r\"^\\s*,\\s*\", \"\", improved_text)\n",
        "\n",
        "    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã, –∏—Å–ø—Ä–∞–≤–ª—è–µ–º\n",
        "    if improved_text and improved_text[0].islower():\n",
        "        improved_text = improved_text[0].upper() + improved_text[1:]\n",
        "\n",
        "    return improved_text\n",
        "\n",
        "def translate_text(text, translator_tokenizer=None, translator_model=None, bert_tokenizer=None, bert_model=None, device=None):\n",
        "    if not text.strip():\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "    except LangDetectException:\n",
        "        lang = 'ru'\n",
        "\n",
        "    # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–∏–º —Ü–µ–ª–∏–∫–æ–º\n",
        "    if len(text) < 500:\n",
        "        inputs = translator_tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            translated = translator_model.generate(\n",
        "                **inputs,\n",
        "                max_length=600,\n",
        "                num_beams=5,\n",
        "                early_stopping=True,\n",
        "                no_repeat_ngram_size=2,\n",
        "                length_penalty=1.0\n",
        "            )\n",
        "\n",
        "        result = translator_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "        # –£–ª—É—á—à–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥ —Å –ø–æ–º–æ—â—å—é BERT\n",
        "        if bert_model and bert_tokenizer:\n",
        "            result = improve_translation_with_bert(result, bert_tokenizer, bert_model, device)\n",
        "\n",
        "        return result\n",
        "\n",
        "    # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ —á–∞—Å—Ç–∏\n",
        "    try:\n",
        "        sentences = sent_tokenize(text, language=lang)\n",
        "    except (LookupError, ValueError):\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "    translated_parts = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent = sent.strip()\n",
        "        if not sent:\n",
        "            continue\n",
        "\n",
        "        # –°–æ–±–∏—Ä–∞–µ–º —á–∞–Ω–∫ –Ω–µ –±–æ–ª–µ–µ 400 —Å–∏–º–≤–æ–ª–æ–≤\n",
        "        if len(current_chunk) + len(sent) + 1 < 400:\n",
        "            current_chunk += f\" {sent}\" if current_chunk else sent\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                # –ü–µ—Ä–µ–≤–æ–¥–∏–º —á–∞–Ω–∫\n",
        "                inputs = translator_tokenizer(\n",
        "                    current_chunk,\n",
        "                    return_tensors=\"pt\",\n",
        "                    truncation=True,\n",
        "                    max_length=512\n",
        "                ).to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    translated = translator_model.generate(\n",
        "                        **inputs,\n",
        "                        max_length=600,\n",
        "                        num_beams=5,\n",
        "                        early_stopping=True,\n",
        "                        no_repeat_ngram_size=2,\n",
        "                        length_penalty=1.0\n",
        "                    )\n",
        "\n",
        "                translated_part = translator_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "                # –£–ª—É—á—à–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥ —Å –ø–æ–º–æ—â—å—é BERT\n",
        "                if bert_model and bert_tokenizer:\n",
        "                    translated_part = improve_translation_with_bert(translated_part, bert_tokenizer, bert_model, device)\n",
        "\n",
        "                translated_parts.append(translated_part)\n",
        "            current_chunk = sent\n",
        "\n",
        "    # –ù–µ –∑–∞–±—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫\n",
        "    if current_chunk:\n",
        "        inputs = translator_tokenizer(\n",
        "            current_chunk,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            translated = translator_model.generate(\n",
        "                **inputs,\n",
        "                max_length=600,\n",
        "                num_beams=5,\n",
        "                early_stopping=True,\n",
        "                no_repeat_ngram_size=2,\n",
        "                length_penalty=1.0\n",
        "            )\n",
        "\n",
        "        translated_part = translator_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "        # –£–ª—É—á—à–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥ —Å –ø–æ–º–æ—â—å—é BERT\n",
        "        if bert_model and bert_tokenizer:\n",
        "            translated_part = improve_translation_with_bert(translated_part, bert_tokenizer, bert_model, device)\n",
        "\n",
        "        translated_parts.append(translated_part)\n",
        "\n",
        "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–µ —á–∞—Å—Ç–∏\n",
        "    result = \" \".join(translated_parts)\n",
        "\n",
        "    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
        "    if bert_model and bert_tokenizer:\n",
        "        result = improve_translation_with_bert(result, bert_tokenizer, bert_model, device)\n",
        "\n",
        "    return result\n",
        "\n",
        "def evaluate_simplification(orig, simp):\n",
        "    orig_words = set(orig.lower().split())\n",
        "    simp_words = set(simp.lower().split())\n",
        "\n",
        "    keyword_overlap = len(orig_words & simp_words) / len(orig_words) * 100 if orig_words else 0\n",
        "    compression = round(len(simp.split()) / len(orig.split()) * 100, 1) if orig.split() else 0\n",
        "\n",
        "    orig_complexity = sum(len(word) for word in orig.split()) / len(orig.split()) if orig.split() else 0\n",
        "    simp_complexity = sum(len(word) for word in simp.split()) / len(simp.split()) if simp.split() else 0\n",
        "\n",
        "    return {\n",
        "        \"original_length\": len(orig.split()),\n",
        "        \"simplified_length\": len(simp.split()),\n",
        "        \"compression_%\": compression,\n",
        "        \"keyword_overlap_%\": round(keyword_overlap, 1),\n",
        "        \"complexity_reduction\": round(orig_complexity - simp_complexity, 2),\n",
        "        \"quality_hint\":\n",
        "            \"üü¢ –û—Ç–ª–∏—á–Ω–æ–µ —É–ø—Ä–æ—â–µ–Ω–∏–µ\" if keyword_overlap > 70 and compression < 80 else\n",
        "            \"üü° –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\" if keyword_overlap > 50 else\n",
        "            \"üî¥ –ü–ª–æ—Ö–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–º—ã—Å–ª–∞\"\n",
        "    }\n",
        "\n",
        "# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---\n",
        "async def send_typing_action(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    await context.bot.send_chat_action(\n",
        "        chat_id=update.effective_chat.id,\n",
        "        action=\"typing\"\n",
        "    )\n",
        "\n",
        "async def safe_delete_file(file_path: str):\n",
        "    try:\n",
        "        if os.path.exists(file_path):\n",
        "            os.remove(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}\")\n",
        "\n",
        "async def read_txt_file(file_path: str) -> Optional[str]:\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            raw_data = f.read()\n",
        "\n",
        "        result = chardet.detect(raw_data)\n",
        "        encoding = result['encoding'] or 'utf-8'\n",
        "\n",
        "        return raw_data.decode(encoding, errors='replace')\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è txt —Ñ–∞–π–ª–∞: {e}\")\n",
        "        return None\n",
        "\n",
        "async def read_docx_file(file_path: str) -> Optional[str]:\n",
        "    try:\n",
        "        doc = Document(file_path)\n",
        "        return \"\\n\".join(p.text for p in doc.paragraphs if p.text.strip())\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è docx —Ñ–∞–π–ª–∞: {e}\")\n",
        "        return None\n",
        "\n",
        "async def send_thinking_messages(query, messages: list, delay: float = 1.2):\n",
        "    for msg in messages:\n",
        "        try:\n",
        "            if msg != query.message.text:\n",
        "                await query.edit_message_text(msg)\n",
        "            await asyncio.sleep(delay)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error sending thinking message: {e}\")\n",
        "\n",
        "async def safe_edit_message(query, text: str, reply_markup=None, parse_mode=None):\n",
        "    try:\n",
        "        await query.edit_message_text(\n",
        "            text=text,\n",
        "            reply_markup=reply_markup,\n",
        "            parse_mode=parse_mode\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error editing message: {e}\")\n",
        "        try:\n",
        "            await query.message.reply_text(\n",
        "                text=text,\n",
        "                reply_markup=reply_markup,\n",
        "                parse_mode=parse_mode\n",
        "            )\n",
        "        except Exception as fallback_error:\n",
        "            logger.error(f\"Fallback error: {fallback_error}\")\n",
        "\n",
        "def get_simplify_keyboard() -> InlineKeyboardMarkup:\n",
        "    # –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞ –±–µ–∑ –∫–Ω–æ–ø–∫–∏ \"–§–∞–∫—Ç—á–µ–∫–∏–Ω–≥\"\n",
        "    keyboard = [\n",
        "        [InlineKeyboardButton(\"üî§ –ü–µ—Ä–µ–≤–µ—Å—Ç–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π\", callback_data=\"translate\")],\n",
        "        [InlineKeyboardButton(\"üîÑ –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π —É—Ä–æ–≤–µ–Ω—å\", callback_data=\"change_level\")],\n",
        "        [InlineKeyboardButton(\"üìÑ –ü–æ–∫–∞–∑–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª\", callback_data=\"show_original\")]\n",
        "    ]\n",
        "    return InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "# --- –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ Telegram-–±–æ—Ç–∞ ---\n",
        "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    await send_typing_action(update, context)\n",
        "\n",
        "    await update.message.reply_text(\n",
        "        \"üëã –ü—Ä–∏–≤–µ—Ç! –Ø ‚Äî *TextEaseBot*.\\n\\n\"\n",
        "        \"üìå –Ø –ø–æ–º–æ–≥–∞—é:\\n\"\n",
        "        \"‚Ä¢ –£–ø—Ä–æ—â–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã\\n\"\n",
        "        \"‚Ä¢ –ü–µ—Ä–µ–≤–æ–¥–∏—Ç—å –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π\\n\"\n",
        "        \"üí° *–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:*\\n\"\n",
        "        \"1. –û—Ç–ø—Ä–∞–≤—å —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é\\n\"\n",
        "        \"2. –ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏ —Ñ–∞–π–ª (.txt, .docx)\\n\"\n",
        "        \"3. –í—ã–±–µ—Ä–∏ –Ω—É–∂–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é\\n\\n\"\n",
        "        \"‚ö†Ô∏è *–í–∞–∂–Ω–æ:*\\n\"\n",
        "        \"‚Ä¢ –Ø ‚Äî –ò–ò, –∞ –Ω–µ —ç–∫—Å–ø–µ—Ä—Ç\\n\"\n",
        "        \"‚Ä¢ –ú–æ–≥—É –æ—à–∏–±–∞—Ç—å—Å—è –≤ —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–º–∞—Ö\\n\"\n",
        "        \"‚Ä¢ –ù–µ –∑–∞–º–µ–Ω—è—é –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É\\n\\n\"\n",
        "        \"üîç –î–ª—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–π —Ä–µ–∂–∏–º *–§–∞–∫—Ç—á–µ–∫–∏–Ω–≥*.\\n\\n\"\n",
        "        \"–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ? –ü—Ä–∏—Å—ã–ª–∞–π —Ç–µ–∫—Å—Ç!\",\n",
        "        parse_mode='Markdown',\n",
        "        disable_web_page_preview=True\n",
        "    )\n",
        "\n",
        "    await show_buttons(update, context)\n",
        "\n",
        "def get_main_keyboard() -> InlineKeyboardMarkup:\n",
        "    \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É –≥–ª–∞–≤–Ω–æ–≥–æ –º–µ–Ω—é\"\"\"\n",
        "    keyboard = [\n",
        "        [\n",
        "            InlineKeyboardButton(\"‚öñÔ∏è –°—Ä–µ–¥–Ω–µ–µ —É–ø—Ä–æ—â–µ–Ω–∏–µ\", callback_data=\"simplify_medium\"),\n",
        "            InlineKeyboardButton(\"üî• –°–∏–ª—å–Ω–æ–µ —É–ø—Ä–æ—â–µ–Ω–∏–µ\", callback_data=\"simplify_strong\")\n",
        "        ],\n",
        "        [\n",
        "            InlineKeyboardButton(\"üåç –ü–µ—Ä–µ–≤–æ–¥\", callback_data=\"translate\"),\n",
        "            InlineKeyboardButton(\"üîç –§–∞–∫—Ç—á–µ–∫–∏–Ω–≥\", callback_data=\"fact_checking\")\n",
        "        ],\n",
        "        [\n",
        "            InlineKeyboardButton(\"‚ÑπÔ∏è –ü–æ–º–æ—â—å\", callback_data=\"help\")\n",
        "        ]\n",
        "    ]\n",
        "    return InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "async def show_buttons(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    \"\"\"–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é\"\"\"\n",
        "    keyboard = get_main_keyboard()\n",
        "\n",
        "    await update.message.reply_text(\n",
        "        \"–í—ã–±–µ—Ä–∏ –¥–µ–π—Å—Ç–≤–∏–µ:\",\n",
        "        reply_markup=keyboard\n",
        "    )\n",
        "\n",
        "async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    await send_typing_action(update, context)\n",
        "\n",
        "    text = update.message.text.strip()\n",
        "    if not text:\n",
        "        await update.message.reply_text(\"‚ùå –ü—É—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –û—Ç–ø—Ä–∞–≤—å —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ñ–∞–π–ª.\")\n",
        "        return\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞\n",
        "    if len(text) > MAX_TEXT_LENGTH:\n",
        "        await update.message.reply_text(\n",
        "            f\"‚ùå –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤).\\n\"\n",
        "            f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {MAX_TEXT_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤.\\n\\n\"\n",
        "            \"üí° –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–æ–∫—Ä–∞—Ç–∏—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ –Ω–∞ —á–∞—Å—Ç–∏.\"\n",
        "        )\n",
        "        return\n",
        "\n",
        "    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ\n",
        "    estimated_parts = len(text) // 1500 + 1\n",
        "    if estimated_parts > MAX_PARTS_FOR_WARNING:\n",
        "        await update.message.reply_text(\n",
        "            f\"‚ö†Ô∏è –¢–µ–∫—Å—Ç –¥–æ–≤–æ–ª—å–Ω–æ –¥–ª–∏–Ω–Ω—ã–π ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤).\\n\"\n",
        "            f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å {estimated_parts * 5} —Å–µ–∫—É–Ω–¥.\\n\"\n",
        "            f\"–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å?\"\n",
        "        )\n",
        "\n",
        "    context.user_data['pending_text'] = text\n",
        "    context.user_data['source_type'] = 'text'\n",
        "\n",
        "    await update.message.reply_text(\n",
        "        f\"üìù –ü–æ–ª—É—á–µ–Ω —Ç–µ–∫—Å—Ç ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤).\\n\"\n",
        "        \"–í—ã–±–µ—Ä–∏ –¥–µ–π—Å—Ç–≤–∏–µ:\",\n",
        "        reply_markup=InlineKeyboardMarkup([\n",
        "            [InlineKeyboardButton(\"‚öñÔ∏è –°—Ä–µ–¥–Ω–∏–π\", callback_data=\"simplify_medium\")],\n",
        "            [InlineKeyboardButton(\"üî• –°–∏–ª—å–Ω—ã–π\", callback_data=\"simplify_strong\")],\n",
        "            [InlineKeyboardButton(\"üîç –§–∞–∫—Ç—á–µ–∫–∏–Ω–≥\", callback_data=\"fact_checking\")]\n",
        "        ])\n",
        "    )\n",
        "\n",
        "async def handle_document(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    await send_typing_action(update, context)\n",
        "\n",
        "    doc = update.message.document\n",
        "    file_name = doc.file_name.lower()\n",
        "    user_id = update.effective_user.id\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞\n",
        "    if not any(file_name.endswith(ext) for ext in {\".txt\", \".docx\"}):\n",
        "        await update.message.reply_text(\n",
        "            \"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç.\\n\"\n",
        "            \"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: .txt, .docx\"\n",
        "        )\n",
        "        return\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞\n",
        "    if doc.file_size > MAX_FILE_SIZE:\n",
        "        await update.message.reply_text(\n",
        "            f\"‚ùå –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª.\\n\"\n",
        "            f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {MAX_FILE_SIZE // (1024*1024)} –ú–ë\"\n",
        "        )\n",
        "        return\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(\n",
        "        delete=False,\n",
        "        suffix=f\"_{user_id}{os.path.splitext(file_name)[1]}\"\n",
        "    ) as temp_file:\n",
        "        temp_path = temp_file.name\n",
        "\n",
        "    try:\n",
        "        file = await doc.get_file()\n",
        "        await file.download_to_drive(temp_path)\n",
        "\n",
        "        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞\n",
        "        if file_name.endswith(\".txt\"):\n",
        "            text = await read_txt_file(temp_path)\n",
        "        elif file_name.endswith(\".docx\"):\n",
        "            text = await read_docx_file(temp_path)\n",
        "        else:\n",
        "            text = None\n",
        "\n",
        "        if text is None:\n",
        "            await update.message.reply_text(\n",
        "                \"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞.\\n\"\n",
        "                \"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–∞ –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        if not text.strip():\n",
        "            await update.message.reply_text(\"‚ùå –§–∞–π–ª –ø—É—Å—Ç–æ–π.\")\n",
        "            return\n",
        "\n",
        "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞\n",
        "        if len(text) > MAX_TEXT_LENGTH:\n",
        "            await update.message.reply_text(\n",
        "                f\"‚ùå –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤).\\n\"\n",
        "                f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {MAX_TEXT_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤.\\n\\n\"\n",
        "                \"üí° –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–æ–∫—Ä–∞—Ç–∏—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ –Ω–∞ —á–∞—Å—Ç–∏.\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ\n",
        "        estimated_parts = len(text) // 1500 + 1\n",
        "        if estimated_parts > MAX_PARTS_FOR_WARNING:\n",
        "            await update.message.reply_text(\n",
        "                f\"‚ö†Ô∏è –¢–µ–∫—Å—Ç –¥–æ–≤–æ–ª—å–Ω–æ –¥–ª–∏–Ω–Ω—ã–π ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤).\\n\"\n",
        "                f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å {estimated_parts * 5} —Å–µ–∫—É–Ω–¥.\\n\"\n",
        "                f\"–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å?\"\n",
        "            )\n",
        "\n",
        "        context.user_data['pending_text'] = text\n",
        "        context.user_data['source_type'] = 'document'\n",
        "\n",
        "        await update.message.reply_text(\n",
        "            f\"üìÑ –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω:\\n\"\n",
        "            f\"‚Ä¢ –ò–º—è: {doc.file_name}\\n\"\n",
        "            f\"‚Ä¢ –†–∞–∑–º–µ—Ä: {doc.file_size // 1024} –ö–ë\\n\"\n",
        "            f\"‚Ä¢ –¢–µ–∫—Å—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤\\n\\n\"\n",
        "            \"–í—ã–±–µ—Ä–∏ –¥–µ–π—Å—Ç–≤–∏–µ:\",\n",
        "            reply_markup=InlineKeyboardMarkup([\n",
        "                [InlineKeyboardButton(\"‚öñÔ∏è –°—Ä–µ–¥–Ω–∏–π\", callback_data=\"simplify_medium\")],\n",
        "                [InlineKeyboardButton(\"üî• –°–∏–ª—å–Ω—ã–π\", callback_data=\"simplify_strong\")],\n",
        "                [InlineKeyboardButton(\"üîç –§–∞–∫—Ç—á–µ–∫–∏–Ω–≥\", callback_data=\"fact_checking\")]\n",
        "            ])\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}\")\n",
        "        await update.message.reply_text(\n",
        "            \"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞.\\n\"\n",
        "            \"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –µ–≥–æ —Å–Ω–æ–≤–∞ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É.\"\n",
        "        )\n",
        "\n",
        "    finally:\n",
        "        await safe_delete_file(temp_path)\n",
        "\n",
        "async def fact_checking_mode(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    query = update.callback_query\n",
        "    await query.answer()\n",
        "\n",
        "    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=\"typing\")\n",
        "\n",
        "    text = context.user_data.get('pending_text', '').strip()\n",
        "    if not text:\n",
        "        await safe_edit_message(query, \"‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ñ–∞–π–ª —Å–Ω–∞—á–∞–ª–∞.\")\n",
        "        return\n",
        "\n",
        "    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
        "    if len(text) > 8000:\n",
        "        await safe_edit_message(query,\n",
        "            f\"‚ö†Ô∏è –¢–µ–∫—Å—Ç –¥–æ–≤–æ–ª—å–Ω–æ –¥–ª–∏–Ω–Ω—ã–π ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤).\\n\"\n",
        "            f\"–í—ã–¥–µ–ª–µ–Ω–∏–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è...\\n\\n\"\n",
        "            f\"–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å?\"\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        sentences = sent_tokenize(text, language='russian')\n",
        "    except (LookupError, AttributeError) as e:\n",
        "        logger.warning(f\"Tokenizer error: {e}\")\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "    claims = [s.strip() for s in sentences if s.strip() and len(s) > 10]\n",
        "\n",
        "    if not claims:\n",
        "        await safe_edit_message(query, \"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–¥–µ–ª–∏—Ç—å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.\")\n",
        "        return\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
        "    context.user_data['fact_check_claims'] = claims\n",
        "\n",
        "    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π\n",
        "    MAX_CLAIMS_DISPLAY = 15\n",
        "    display_claims = claims[:MAX_CLAIMS_DISPLAY]\n",
        "    remaining = len(claims) - MAX_CLAIMS_DISPLAY\n",
        "\n",
        "    header = \"üîç *–†–µ–∂–∏–º —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞:*\\n\\n\"\n",
        "    if remaining > 0:\n",
        "        header += f\"–í—ã–¥–µ–ª–µ–Ω–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π: {len(claims)} (–ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ {MAX_CLAIMS_DISPLAY}):\\n\\n\"\n",
        "    else:\n",
        "        header += f\"–í—ã–¥–µ–ª–µ–Ω–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π: {len(claims)}:\\n\\n\"\n",
        "\n",
        "    await query.edit_message_text(header, parse_mode='Markdown')\n",
        "\n",
        "    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –ø–æ—Ä—Ü–∏—è–º–∏\n",
        "    batch_size = 5\n",
        "    for i in range(0, len(display_claims), batch_size):\n",
        "        batch = display_claims[i:i+batch_size]\n",
        "\n",
        "        for j, claim in enumerate(batch, i+1):\n",
        "            await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=\"typing\")\n",
        "\n",
        "            formatted_claim = f\"_{j}._ {claim}\"\n",
        "\n",
        "            # –¢–æ–ª—å–∫–æ –∫–Ω–æ–ø–∫–∞ \"–£–ø—Ä–æ—Å—Ç–∏—Ç—å\" –±–µ–∑ –∫–Ω–æ–ø–∫–∏ \"–ü—Ä–æ–≤–µ—Ä–∏—Ç—å\"\n",
        "            keyboard = [\n",
        "                [InlineKeyboardButton(\"üìù –£–ø—Ä–æ—Å—Ç–∏—Ç—å\", callback_data=f\"simplify_claim_{j-1}\")]\n",
        "            ]\n",
        "            reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "            try:\n",
        "                await query.message.reply_text(\n",
        "                    formatted_claim,\n",
        "                    parse_mode='Markdown',\n",
        "                    reply_markup=reply_markup\n",
        "                )\n",
        "            except Exception as e:\n",
        "                logger.error(f\"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: {e}\")\n",
        "                await query.message.reply_text(formatted_claim, parse_mode='Markdown')\n",
        "\n",
        "        if i + batch_size < len(display_claims):\n",
        "            await asyncio.sleep(1)\n",
        "\n",
        "    if remaining > 0:\n",
        "        await query.message.reply_text(\n",
        "            f\"‚ÑπÔ∏è –ü–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ {MAX_CLAIMS_DISPLAY} —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π. \"\n",
        "            f\"–û—Å—Ç–∞–ª—å–Ω—ã–µ {remaining} –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–∑–∂–µ.\"\n",
        "        )\n",
        "\n",
        "    # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞ —Å –∫–Ω–æ–ø–∫–æ–π \"–ù–∞–∑–∞–¥ –∫ —Ç–µ–∫—Å—Ç—É\"\n",
        "    keyboard = [\n",
        "        [\n",
        "            InlineKeyboardButton(\"‚¨ÖÔ∏è –ù–∞–∑–∞–¥ –∫ —Ç–µ–∫—Å—Ç—É\", callback_data=\"back_to_uploaded_text\")\n",
        "        ],\n",
        "        [\n",
        "            InlineKeyboardButton(\"‚ùì –ü–æ–º–æ—â—å\", callback_data=\"fact_check_help\")\n",
        "        ]\n",
        "    ]\n",
        "    reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "    await query.message.reply_text(\n",
        "        \"–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:\",\n",
        "        reply_markup=reply_markup\n",
        "    )\n",
        "\n",
        "async def simplify_claim(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    query = update.callback_query\n",
        "    await query.answer()\n",
        "\n",
        "    claim_index = int(query.data.split('_')[-1])\n",
        "    claims = context.user_data.get('fact_check_claims', [])\n",
        "\n",
        "    if not claims or claim_index >= len(claims):\n",
        "        await query.edit_message_text(\"‚ùå –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\")\n",
        "        return\n",
        "\n",
        "    claim = claims[claim_index]\n",
        "\n",
        "    await query.edit_message_text(\n",
        "        f\"üìù *–£–ø—Ä–æ—â–µ–Ω–∏–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è:*\\n\\n_{claim}_\\n\\n\"\n",
        "        \"‚è≥ –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —É–ø—Ä–æ—â–µ–Ω–∏–µ...\",\n",
        "        parse_mode='Markdown'\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å —É–ø—Ä–æ—â–µ–Ω–∏—è —Å —É—Ä–æ–≤–Ω–µ–º medium –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
        "        simplified = simplify_text(\n",
        "            claim,\n",
        "            strength=\"medium\",\n",
        "            simplify_tokenizer=simplify_tokenizer,\n",
        "            simplify_model=simplify_model,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        result_text = (\n",
        "            f\"üìù *–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ:*\\n\\n\"\n",
        "            f\"{simplified}\\n\\n\"\n",
        "            f\"üìå *–û—Ä–∏–≥–∏–Ω–∞–ª:*\\n_{claim}_\"\n",
        "        )\n",
        "\n",
        "        keyboard = [\n",
        "            [\n",
        "                InlineKeyboardButton(\"üîÑ –î—Ä—É–≥–æ–π —É—Ä–æ–≤–µ–Ω—å\", callback_data=f\"change_claim_level_{claim_index}\"),\n",
        "                InlineKeyboardButton(\"‚¨ÖÔ∏è –ù–∞–∑–∞–¥\", callback_data=\"back_to_uploaded_text\")\n",
        "            ]\n",
        "        ]\n",
        "        reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "        await query.edit_message_text(\n",
        "            result_text,\n",
        "            parse_mode='Markdown',\n",
        "            reply_markup=reply_markup\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"–û—à–∏–±–∫–∞ —É–ø—Ä–æ—â–µ–Ω–∏—è —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: {e}\")\n",
        "        await query.edit_message_text(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ø—Ä–æ—â–µ–Ω–∏–∏ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: {e}\")\n",
        "\n",
        "async def change_claim_level(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    query = update.callback_query\n",
        "    await query.answer()\n",
        "\n",
        "    claim_index = int(query.data.split('_')[-1])\n",
        "    claims = context.user_data.get('fact_check_claims', [])\n",
        "\n",
        "    if not claims or claim_index >= len(claims):\n",
        "        await query.edit_message_text(\"‚ùå –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\")\n",
        "        return\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "    context.user_data['current_claim_index'] = claim_index\n",
        "\n",
        "    keyboard = [\n",
        "        [\n",
        "            InlineKeyboardButton(\"‚öñÔ∏è –°—Ä–µ–¥–Ω–∏–π\", callback_data=f\"simplify_claim_medium_{claim_index}\"),\n",
        "            InlineKeyboardButton(\"üî• –°–∏–ª—å–Ω—ã–π\", callback_data=f\"simplify_claim_strong_{claim_index}\")\n",
        "        ],\n",
        "        [\n",
        "            InlineKeyboardButton(\"‚¨ÖÔ∏è –ù–∞–∑–∞–¥\", callback_data=\"back_to_uploaded_text\")\n",
        "        ]\n",
        "    ]\n",
        "    reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "    await query.edit_message_text(\n",
        "        \"–í—ã–±–µ—Ä–∏ —É—Ä–æ–≤–µ–Ω—å —É–ø—Ä–æ—â–µ–Ω–∏—è:\",\n",
        "        reply_markup=reply_markup\n",
        "    )\n",
        "\n",
        "async def show_last_uploaded_text(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    query = update.callback_query\n",
        "    text = context.user_data.get('pending_text', '').strip()\n",
        "\n",
        "    if not text:\n",
        "        await safe_edit_message(query, \"‚ùå –ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\")\n",
        "        return\n",
        "\n",
        "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
        "    try:\n",
        "        await query.delete_message()\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error deleting message: {e}\")\n",
        "\n",
        "    max_length = 4096 - 100\n",
        "    if len(text) <= max_length:\n",
        "        await query.message.reply_text(\n",
        "            f\"üìú *–ü–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:*\\n\\n{text}\",\n",
        "            parse_mode='Markdown'\n",
        "        )\n",
        "    else:\n",
        "        parts = split_text(text, max_chars=3500)\n",
        "        for i, part in enumerate(parts):\n",
        "            if part.strip():\n",
        "                text_part = f\"üìú *–ü–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (—á–∞—Å—Ç—å {i+1}):*\\n\\n{part}\"\n",
        "                await query.message.reply_text(text_part, parse_mode='Markdown')\n",
        "                await asyncio.sleep(0.5)\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –∫–Ω–æ–ø–∫–∏ –¥–ª—è –¥–µ–π—Å—Ç–≤–∏–π —Å —Ç–µ–∫—Å—Ç–æ–º\n",
        "    keyboard = [\n",
        "        [InlineKeyboardButton(\"‚öñÔ∏è –°—Ä–µ–¥–Ω–∏–π\", callback_data=\"simplify_medium\")],\n",
        "        [InlineKeyboardButton(\"üî• –°–∏–ª—å–Ω—ã–π\", callback_data=\"simplify_strong\")],\n",
        "        [InlineKeyboardButton(\"üîç –§–∞–∫—Ç—á–µ–∫–∏–Ω–≥\", callback_data=\"fact_checking\")]\n",
        "    ]\n",
        "    reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "    await query.message.reply_text(\n",
        "        \"–í—ã–±–µ—Ä–∏ –¥–µ–π—Å—Ç–≤–∏–µ:\",\n",
        "        reply_markup=reply_markup\n",
        "    )\n",
        "\n",
        "async def simplify_claim_with_strength(update: Update, context: ContextTypes.DEFAULT_TYPE, claim_index: int, strength: str):\n",
        "    query = update.callback_query\n",
        "    await query.answer()\n",
        "\n",
        "    claims = context.user_data.get('fact_check_claims', [])\n",
        "\n",
        "    if not claims or claim_index >= len(claims):\n",
        "        await query.edit_message_text(\"‚ùå –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\")\n",
        "        return\n",
        "\n",
        "    claim = claims[claim_index]\n",
        "\n",
        "    await query.edit_message_text(\n",
        "        f\"üìù *–£–ø—Ä–æ—â–µ–Ω–∏–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è ({strength}):*\\n\\n_{claim}_\\n\\n\"\n",
        "        \"‚è≥ –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —É–ø—Ä–æ—â–µ–Ω–∏–µ...\",\n",
        "        parse_mode='Markdown'\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        simplified = simplify_text(\n",
        "            claim,\n",
        "            strength=strength,\n",
        "            simplify_tokenizer=simplify_tokenizer,\n",
        "            simplify_model=simplify_model,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        result_text = (\n",
        "            f\"üìù *–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ ({strength}):*\\n\\n\"\n",
        "            f\"{simplified}\\n\\n\"\n",
        "            f\"üìå *–û—Ä–∏–≥–∏–Ω–∞–ª:*\\n_{claim}_\"\n",
        "        )\n",
        "\n",
        "        keyboard = [\n",
        "            [\n",
        "                InlineKeyboardButton(\"üîÑ –î—Ä—É–≥–æ–π —É—Ä–æ–≤–µ–Ω—å\", callback_data=f\"change_claim_level_{claim_index}\"),\n",
        "                InlineKeyboardButton(\"‚¨ÖÔ∏è –ù–∞–∑–∞–¥\", callback_data=\"back_to_uploaded_text\")\n",
        "            ]\n",
        "        ]\n",
        "        reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "        await query.edit_message_text(\n",
        "            result_text,\n",
        "            parse_mode='Markdown',\n",
        "            reply_markup=reply_markup\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"–û—à–∏–±–∫–∞ —É–ø—Ä–æ—â–µ–Ω–∏—è —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: {e}\")\n",
        "        await query.edit_message_text(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ø—Ä–æ—â–µ–Ω–∏–∏ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: {e}\")\n",
        "\n",
        "async def fact_check_help(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    query = update.callback_query\n",
        "    await query.answer()\n",
        "\n",
        "    help_text = (\n",
        "        \"‚ùì *–ü–æ–º–æ—â—å –ø–æ —Ä–µ–∂–∏–º—É —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞*\\n\\n\"\n",
        "        \"üîç *–ß—Ç–æ —ç—Ç–æ —Ç–∞–∫–æ–µ?*\\n\"\n",
        "        \"–†–µ–∂–∏–º —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤—ã–¥–µ–ª—è–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å.\\n\\n\"\n",
        "        \"üìã *–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:*\\n\"\n",
        "        \"1. –û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª\\n\"\n",
        "        \"2. –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º '–§–∞–∫—Ç—á–µ–∫–∏–Ω–≥'\\n\"\n",
        "        \"3. –ë–æ—Ç —Ä–∞–∑–æ–±—å–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è\\n\"\n",
        "        \"4. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω—ã –¥–µ–π—Å—Ç–≤–∏—è:\\n\"\n",
        "        \"   ‚Ä¢ üìù *–£–ø—Ä–æ—Å—Ç–∏—Ç—å* - —É–ø—Ä–æ—Å—Ç–∏—Ç—å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ\\n\\n\"\n",
        "        \"‚ö†Ô∏è *–í–∞–∂–Ω–æ:*\\n\"\n",
        "        \"‚Ä¢ –ë–æ—Ç –Ω–µ –∑–∞–º–µ–Ω—è–µ—Ç –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É\\n\"\n",
        "        \"‚Ä¢ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Å—è—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä\\n\"\n",
        "        \"‚Ä¢ –î–ª—è –≤–∞–∂–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –≤—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\\n\\n\"\n",
        "        \"üí° *–°–æ–≤–µ—Ç:* –ù–∞—á–Ω–∏—Ç–µ —Å –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –∏–ª–∏ —Å–ø–æ—Ä–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π.\"\n",
        "    )\n",
        "\n",
        "    keyboard = [\n",
        "        [\n",
        "            InlineKeyboardButton(\"‚¨ÖÔ∏è –ù–∞–∑–∞–¥ –∫ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è–º\", callback_data=\"back_to_fact_check\")\n",
        "        ]\n",
        "    ]\n",
        "    reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "    await query.edit_message_text(\n",
        "        help_text,\n",
        "        parse_mode='Markdown',\n",
        "        reply_markup=reply_markup\n",
        "    )\n",
        "\n",
        "async def show_help(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    query = update.callback_query\n",
        "    await query.answer()\n",
        "\n",
        "    help_text = (\n",
        "        \"‚ùì *–ü–æ–º–æ—â—å –ø–æ TextEaseBot*\\n\\n\"\n",
        "        \"ü§ñ *–ß—Ç–æ —è —É–º–µ—é:*\\n\"\n",
        "        \"‚Ä¢ üìù –£–ø—Ä–æ—â–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω–∞ 2 —É—Ä–æ–≤–Ω—è—Ö\\n\"\n",
        "        \"‚Ä¢ üåç –ü–µ—Ä–µ–≤–æ–¥–∏—Ç—å —Ç–µ–∫—Å—Ç—ã –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫\\n\"\n",
        "        \"‚Ä¢ üìÑ –†–∞–±–æ—Ç–∞—Ç—å —Å —Ñ–∞–π–ª–∞–º–∏ .txt –∏ .docx\\n\\n\"\n",
        "        \"üìã *–ö–∞–∫ –Ω–∞—á–∞—Ç—å:*\\n\"\n",
        "        \"1. –û—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é\\n\"\n",
        "        \"2. –ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å —Ç–µ–∫—Å—Ç–æ–º\\n\"\n",
        "        \"3. –í—ã–±–µ—Ä–∏—Ç–µ –Ω—É–∂–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ –º–µ–Ω—é\\n\\n\"\n",
        "        \"üîß *–£—Ä–æ–≤–Ω–∏ —É–ø—Ä–æ—â–µ–Ω–∏—è:*\\n\"\n",
        "        \"‚Ä¢ ‚öñÔ∏è *–°—Ä–µ–¥–Ω–∏–π* - –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —É–ø—Ä–æ—â–µ–Ω–∏–µ\\n\"\n",
        "        \"‚Ä¢ üî• *–°–∏–ª—å–Ω—ã–π* - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —É–ø—Ä–æ—â–µ–Ω–∏–µ\\n\\n\"\n",
        "        \"‚ö†Ô∏è *–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:*\\n\"\n",
        "        \"‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: 20 –ú–ë\\n\"\n",
        "        \"‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: 10 000 —Å–∏–º–≤–æ–ª–æ–≤\\n\"\n",
        "        \"‚Ä¢ –Ø - –ò–ò, –º–æ–≥—É –æ—à–∏–±–∞—Ç—å—Å—è –≤ —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–º–∞—Ö\\n\\n\"\n",
        "        \"üí° *–°–æ–≤–µ—Ç:* –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–∞–π–ª—ã.\"\n",
        "    )\n",
        "\n",
        "    keyboard = [\n",
        "        [\n",
        "            InlineKeyboardButton(\"‚¨ÖÔ∏è –ù–∞–∑–∞–¥\", callback_data=\"back_to_main\"),\n",
        "            InlineKeyboardButton(\"üîÑ –ù–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ\", callback_data=\"restart\")\n",
        "        ]\n",
        "    ]\n",
        "    reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "    await query.edit_message_text(\n",
        "        help_text,\n",
        "        parse_mode='Markdown',\n",
        "        reply_markup=reply_markup\n",
        "    )\n",
        "\n",
        "async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    await send_typing_action(update, context)\n",
        "\n",
        "    help_text = (\n",
        "        \"‚ùì *–ü–æ–º–æ—â—å –ø–æ TextEaseBot*\\n\\n\"\n",
        "        \"ü§ñ *–ß—Ç–æ —è —É–º–µ—é:*\\n\"\n",
        "        \"‚Ä¢ üìù –£–ø—Ä–æ—â–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω–∞ 2 —É—Ä–æ–≤–Ω—è—Ö\\n\"\n",
        "        \"‚Ä¢ üåç –ü–µ—Ä–µ–≤–æ–¥–∏—Ç—å —Ç–µ–∫—Å—Ç—ã –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫\\n\"\n",
        "        \"‚Ä¢ üìÑ –†–∞–±–æ—Ç–∞—Ç—å —Å —Ñ–∞–π–ª–∞–º–∏ .txt –∏ .docx\\n\\n\"\n",
        "        \"üìã *–ö–∞–∫ –Ω–∞—á–∞—Ç—å:*\\n\"\n",
        "        \"1. –û—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é\\n\"\n",
        "        \"2. –ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å —Ç–µ–∫—Å—Ç–æ–º\\n\"\n",
        "        \"3. –í—ã–±–µ—Ä–∏—Ç–µ –Ω—É–∂–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ –º–µ–Ω—é\\n\\n\"\n",
        "        \"üîß *–£—Ä–æ–≤–Ω–∏ —É–ø—Ä–æ—â–µ–Ω–∏—è:*\\n\"\n",
        "        \"‚Ä¢ ‚öñÔ∏è *–°—Ä–µ–¥–Ω–∏–π* - –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —É–ø—Ä–æ—â–µ–Ω–∏–µ\\n\"\n",
        "        \"‚Ä¢ üî• *–°–∏–ª—å–Ω—ã–π* - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —É–ø—Ä–æ—â–µ–Ω–∏–µ\\n\\n\"\n",
        "        \"‚ö†Ô∏è *–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:*\\n\"\n",
        "        \"‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: 20 –ú–ë\\n\"\n",
        "        \"‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: 10 000 —Å–∏–º–≤–æ–ª–æ–≤\\n\"\n",
        "        \"‚Ä¢ –Ø - –ò–ò, –º–æ–≥—É –æ—à–∏–±–∞—Ç—å—Å—è –≤ —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–º–∞—Ö\\n\\n\"\n",
        "        \"üí° *–°–æ–≤–µ—Ç:* –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–∞–π–ª—ã.\"\n",
        "    )\n",
        "\n",
        "    keyboard = [\n",
        "        [\n",
        "            InlineKeyboardButton(\"üîÑ –ù–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ\", callback_data=\"restart\")\n",
        "        ]\n",
        "    ]\n",
        "    reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "    await update.message.reply_text(\n",
        "        help_text,\n",
        "        parse_mode='Markdown',\n",
        "        reply_markup=reply_markup\n",
        "    )\n",
        "\n",
        "async def handle_back_to_simplified(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    query = update.callback_query\n",
        "    simplified = context.user_data.get('simplified_text', '').strip()\n",
        "    strength = context.user_data.get('last_strength', 'medium')\n",
        "\n",
        "    if not simplified:\n",
        "        await safe_edit_message(query, \"‚ùå –ù–µ—Ç —É–ø—Ä–æ—â—ë–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\")\n",
        "        return\n",
        "\n",
        "    strength_names = {\n",
        "        'light': '–ª—ë–≥–∫–∏–π',\n",
        "        'medium': '—Å—Ä–µ–¥–Ω–∏–π',\n",
        "        'strong': '—Å–∏–ª—å–Ω—ã–π'\n",
        "    }\n",
        "    strength_name = strength_names.get(strength, '—Å—Ä–µ–¥–Ω–∏–π')\n",
        "\n",
        "    await safe_edit_message(\n",
        "        query,\n",
        "        f\"‚úÖ *–£–ø—Ä–æ—â—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç ({strength_name}):*\\n\\n{simplified}\",\n",
        "        reply_markup=get_simplify_keyboard(),\n",
        "        parse_mode='Markdown'\n",
        "    )\n",
        "\n",
        "async def handle_simplify(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    query = update.callback_query\n",
        "    data = query.data\n",
        "    strength = data.split(\"_\")[1]\n",
        "    text = context.user_data.get('pending_text', '').strip()\n",
        "\n",
        "    if not text:\n",
        "        await safe_edit_message(query, \"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è.\")\n",
        "        return\n",
        "\n",
        "    await safe_edit_message(query, f\"üîÑ –£–ø—Ä–æ—â–∞—é —Ç–µ–∫—Å—Ç ({strength} —É—Ä–æ–≤–µ–Ω—å)...\")\n",
        "\n",
        "    if len(text) > 2000:\n",
        "        parts = split_text(text, 1500)\n",
        "        total_parts = len(parts)\n",
        "\n",
        "        for i, part in enumerate(parts):\n",
        "            if part.strip():\n",
        "                progress = (i + 1) / total_parts * 100\n",
        "                await safe_edit_message(\n",
        "                    query,\n",
        "                    f\"üîÑ –£–ø—Ä–æ—â–∞—é —Ç–µ–∫—Å—Ç ({strength} —É—Ä–æ–≤–µ–Ω—å)...\\n\\n\"\n",
        "                    f\"–ü—Ä–æ–≥—Ä–µ—Å—Å: {i+1}/{total_parts} —á–∞—Å—Ç–µ–π ({progress:.0f}%)\"\n",
        "                )\n",
        "                await asyncio.sleep(0.5)\n",
        "\n",
        "    thinking_messages = [\n",
        "        \"üß† –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç–∞...\",\n",
        "        \"‚úçÔ∏è –£–ø—Ä–æ—â–∞—é —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–º—ã—Å–ª–∞...\",\n",
        "        \"üîç –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞—é ‚Äî —á—Ç–æ–±—ã –±—ã–ª–æ —è—Å–Ω–æ...\",\n",
        "        \"üß© –ì–æ—Ç–æ–≤–ª—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç...\",\n",
        "        \"‚úÖ –ü–æ—á—Ç–∏ –≥–æ—Ç–æ–≤–æ!\"\n",
        "    ]\n",
        "\n",
        "    random.shuffle(thinking_messages)\n",
        "    await send_thinking_messages(query, thinking_messages)\n",
        "\n",
        "    try:\n",
        "        simplified = simplify_long_text(\n",
        "            text,\n",
        "            strength=strength,\n",
        "            simplify_tokenizer=simplify_tokenizer,\n",
        "            simplify_model=simplify_model,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        context.user_data['simplified_text'] = simplified\n",
        "        context.user_data['last_strength'] = strength\n",
        "\n",
        "        metrics = evaluate_simplification(text, simplified)\n",
        "\n",
        "        quality_info = (\n",
        "            f\"\\n\\nüìä *–û—Ü–µ–Ω–∫–∞ —É–ø—Ä–æ—â–µ–Ω–∏—è:*\\n\"\n",
        "            f\"üî§ –î–ª–∏–Ω–∞: {metrics['original_length']} ‚Üí {metrics['simplified_length']} —Å–ª–æ–≤\\n\"\n",
        "            f\"‚öñÔ∏è –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–º—ã—Å–ª–∞: {metrics['keyword_overlap_%']}%\\n\"\n",
        "            f\"üí° {metrics['quality_hint']}\"\n",
        "        )\n",
        "\n",
        "        warning = \"\"\n",
        "        if len(simplified.split()) > 300:\n",
        "            warning = \"\\n\\n‚ö†Ô∏è *–í–Ω–∏–º–∞–Ω–∏–µ:* —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω—ã–π ‚Äî –º–æ–≥ –±—ã—Ç—å —á–∞—Å—Ç–∏—á–Ω–æ –æ–±—Ä–µ–∑–∞–Ω.\"\n",
        "\n",
        "        # –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞ –±–µ–∑ –∫–Ω–æ–ø–∫–∏ \"–§–∞–∫—Ç—á–µ–∫–∏–Ω–≥\"\n",
        "        keyboard = [\n",
        "            [InlineKeyboardButton(\"üî§ –ü–µ—Ä–µ–≤–µ—Å—Ç–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π\", callback_data=\"translate\")],\n",
        "            [InlineKeyboardButton(\"üîÑ –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π —É—Ä–æ–≤–µ–Ω—å\", callback_data=\"change_level\")],\n",
        "            [InlineKeyboardButton(\"üìÑ –ü–æ–∫–∞–∑–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª\", callback_data=\"show_original\")]\n",
        "        ]\n",
        "        reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "        await safe_edit_message(\n",
        "            query,\n",
        "            f\"‚úÖ *–£–ø—Ä–æ—â—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç ({strength}):*\\n\\n{simplified}{quality_info}{warning}\",\n",
        "            reply_markup=reply_markup,\n",
        "            parse_mode='Markdown'\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Simplification error: {e}\")\n",
        "        await safe_edit_message(query, f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ø—Ä–æ—â–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}\")\n",
        "\n",
        "async def handle_translate(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    query = update.callback_query\n",
        "    simplified = context.user_data.get('simplified_text', '').strip()\n",
        "\n",
        "    if not simplified:\n",
        "        await safe_edit_message(query, \"‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞.\")\n",
        "        return\n",
        "\n",
        "    await safe_edit_message(query, \"üî§ –ü–µ—Ä–µ–≤–æ–∂—É –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π...\")\n",
        "\n",
        "    try:\n",
        "        translated = translate_text(\n",
        "            simplified,\n",
        "            translator_tokenizer=translator_tokenizer,\n",
        "            translator_model=translator_model,\n",
        "            bert_tokenizer=bert_tokenizer,  # –î–æ–±–∞–≤–ª—è–µ–º BERT\n",
        "            bert_model=bert_model,          # –î–æ–±–∞–≤–ª—è–µ–º BERT\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        keyboard = [\n",
        "            [InlineKeyboardButton(\"‚¨ÖÔ∏è –ù–∞–∑–∞–¥ –∫ —É–ø—Ä–æ—â—ë–Ω–Ω–æ–º—É\", callback_data=\"back_to_simplified\")]\n",
        "        ]\n",
        "        reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "        await safe_edit_message(\n",
        "            query,\n",
        "            f\"üá¨üáß *–ü–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π:*\\n\\n{translated}\",\n",
        "            reply_markup=reply_markup,\n",
        "            parse_mode='Markdown'\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Translation error: {e}\")\n",
        "        await safe_edit_message(query, f\"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}\")\n",
        "\n",
        "async def handle_change_level(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    query = update.callback_query\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç–µ–∫—Å—Ç –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è\n",
        "    if not context.user_data.get('pending_text'):\n",
        "        await safe_edit_message(query, \"‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è. –û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ñ–∞–π–ª —Å–Ω–∞—á–∞–ª–∞.\")\n",
        "        return\n",
        "\n",
        "    await query.message.reply_text(\n",
        "        \"üîÑ *–ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —É–ø—Ä–æ—â–µ–Ω–∏—è*\\n–í—ã–±–µ—Ä–∏ –¥—Ä—É–≥–æ–π –≤–∞—Ä–∏–∞–Ω—Ç:\",\n",
        "        parse_mode='Markdown',\n",
        "        reply_markup=InlineKeyboardMarkup([\n",
        "            # –£–±—Ä–∞–Ω–∞ –∫–Ω–æ–ø–∫–∞ \"–õ—ë–≥–∫–∏–π\"\n",
        "            [InlineKeyboardButton(\"‚öñÔ∏è –°—Ä–µ–¥–Ω–∏–π\", callback_data=\"simplify_medium\")],\n",
        "            [InlineKeyboardButton(\"üî• –°–∏–ª—å–Ω—ã–π\", callback_data=\"simplify_strong\")],\n",
        "            [InlineKeyboardButton(\"üîç –§–∞–∫—Ç—á–µ–∫–∏–Ω–≥\", callback_data=\"fact_checking\")]\n",
        "        ])\n",
        "    )\n",
        "\n",
        "async def handle_show_original(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    query = update.callback_query\n",
        "    original = context.user_data.get('pending_text', '').strip()\n",
        "\n",
        "    if not original:\n",
        "        await query.answer(\"‚ùå –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω\", show_alert=True)\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        await query.delete_message()\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error deleting message: {e}\")\n",
        "\n",
        "    max_length = 4096 - 100\n",
        "\n",
        "    if len(original) <= max_length:\n",
        "        await query.message.reply_text(\n",
        "            f\"üìú *–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç:*\\n\\n{original}\",\n",
        "            parse_mode='Markdown'\n",
        "        )\n",
        "    else:\n",
        "        parts = split_text(original, max_chars=3500)\n",
        "        for i, part in enumerate(parts):\n",
        "            if part.strip():\n",
        "                text = f\"üìú *–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (—á–∞—Å—Ç—å {i+1}):*\\n\\n{part}\"\n",
        "                await query.message.reply_text(text, parse_mode='Markdown')\n",
        "                await asyncio.sleep(0.5)\n",
        "\n",
        "    keyboard = [[InlineKeyboardButton(\"‚¨ÖÔ∏è –ù–∞–∑–∞–¥ –∫ —É–ø—Ä–æ—â—ë–Ω–Ω–æ–º—É\", callback_data=\"back_to_simplified\")]]\n",
        "    reply_markup = InlineKeyboardMarkup(keyboard)\n",
        "\n",
        "    await query.message.reply_text(\n",
        "        \"–•–æ—á–µ—à—å –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ —É–ø—Ä–æ—â—ë–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É?\",\n",
        "        reply_markup=reply_markup\n",
        "    )\n",
        "\n",
        "async def button_click(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    query = update.callback_query\n",
        "    await query.answer()\n",
        "\n",
        "    data = query.data\n",
        "    user_id = update.effective_user.id\n",
        "    logger.info(f\"User {user_id} clicked: {data}\")\n",
        "\n",
        "    if data.startswith(\"simplify_claim_\"):\n",
        "        await simplify_claim(update, context)\n",
        "    elif data.startswith(\"change_claim_level_\"):\n",
        "        await change_claim_level(update, context)\n",
        "    elif data.startswith(\"simplify_claim_medium_\") or data.startswith(\"simplify_claim_strong_\"):\n",
        "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏ —É—Ä–æ–≤–µ–Ω—å\n",
        "        parts = data.split('_')\n",
        "        strength = parts[2]\n",
        "        claim_index = int(parts[3])\n",
        "        await simplify_claim_with_strength(update, context, claim_index, strength)\n",
        "    elif data == \"back_to_uploaded_text\":\n",
        "        await show_last_uploaded_text(update, context)\n",
        "    elif data == \"back_to_fact_check\":\n",
        "        await fact_checking_mode(update, context)\n",
        "    elif data == \"fact_check_help\":\n",
        "        await fact_check_help(update, context)\n",
        "    elif data == \"fact_checking\":\n",
        "        await fact_checking_mode(update, context)\n",
        "    elif data == \"back_to_simplified\":\n",
        "        await handle_back_to_simplified(update, context)\n",
        "    elif data.startswith(\"simplify_\"):\n",
        "        await handle_simplify(update, context)\n",
        "    elif data == \"translate\":\n",
        "        await handle_translate(update, context)\n",
        "    elif data == \"change_level\":\n",
        "        await handle_change_level(update, context)\n",
        "    elif data == \"show_original\":\n",
        "        await handle_show_original(update, context)\n",
        "    elif data == \"back_to_main\":\n",
        "        keyboard = get_main_keyboard()\n",
        "        await query.message.reply_text(\n",
        "            \"–í—ã–±–µ—Ä–∏ –¥–µ–π—Å—Ç–≤–∏–µ:\",\n",
        "            reply_markup=keyboard\n",
        "        )\n",
        "    elif data == \"help\":\n",
        "        await show_help(update, context)\n",
        "    elif data == \"restart\":\n",
        "        await start(update, context)\n",
        "    else:\n",
        "        logger.warning(f\"Unknown callback data: {data}\")\n",
        "        await query.edit_message_text(\"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ\")"
      ],
      "metadata": {
        "id": "_WCjGquah7RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ ---\n",
        "def signal_handler(sig, frame):\n",
        "    \"\"\"–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã\"\"\"\n",
        "    print(\"\\nüõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–æ—Ç–∞...\")\n",
        "    sys.exit(0)\n",
        "\n",
        "async def shutdown(application):\n",
        "    \"\"\"–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è\"\"\"\n",
        "    print(\"üîÑ –ó–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É –±–æ—Ç–∞...\")\n",
        "    try:\n",
        "        await application.updater.stop()\n",
        "        await application.stop()\n",
        "        await application.shutdown()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã: {e}\")\n",
        "\n",
        "async def main():\n",
        "    try:\n",
        "        if not BOT_TOKEN or BOT_TOKEN == \"YOUR_BOT_TOKEN_HERE\":\n",
        "            raise RuntimeError(\"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω –±–æ—Ç–∞\")\n",
        "\n",
        "        application = Application.builder().token(BOT_TOKEN).build()\n",
        "\n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏\n",
        "        application.add_handler(CommandHandler(\"start\", start))\n",
        "        application.add_handler(CommandHandler(\"help\", help_command))\n",
        "        application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\n",
        "        application.add_handler(MessageHandler(filters.Document.ALL, handle_document))\n",
        "        application.add_handler(CallbackQueryHandler(button_click))\n",
        "\n",
        "        print(\"ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω...\")\n",
        "        print(\"üí° –î–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –±–æ—Ç–∞ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C –∏–ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —è—á–µ–π–∫–∏\")\n",
        "\n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∑–∞–ø—É—Å–∫\n",
        "        await application.initialize()\n",
        "        await application.start()\n",
        "        await application.updater.start_polling(drop_pending_updates=True)\n",
        "\n",
        "        # –û–∂–∏–¥–∞–µ–º —Å–∏–≥–Ω–∞–ª–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è\n",
        "        while True:\n",
        "            await asyncio.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        if 'application' in locals():\n",
        "            await shutdown(application)\n",
        "        print(\"üõë –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤\n",
        "    signal.signal(signal.SIGINT, signal_handler)\n",
        "    signal.signal(signal.SIGTERM, signal_handler)\n",
        "\n",
        "    try:\n",
        "        asyncio.run(main())\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–æ—Ç–∞...\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yxlA_mfZ_TLX",
        "outputId": "57b2e683-d3d2-4524-a970-006f8003f9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω...\n",
            "üí° –î–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –±–æ—Ç–∞ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C –∏–ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —è—á–µ–π–∫–∏\n",
            "üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Ä–∞–∑–±–∏—Ç–∞ –Ω–∞ 4 —á–∞—Å—Ç–µ–π –ø–æ 1500 —Å–∏–º–≤–æ–ª–æ–≤\n",
            "üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ 1/4 (1491 —Å–∏–º–≤–æ–ª–æ–≤)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1636: UserWarning: Unfeasible length constraints: `min_length` (512) is larger than the maximum possible length (300). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ 2/4 (1338 —Å–∏–º–≤–æ–ª–æ–≤)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1636: UserWarning: Unfeasible length constraints: `min_length` (478) is larger than the maximum possible length (300). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ 3/4 (1255 —Å–∏–º–≤–æ–ª–æ–≤)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1636: UserWarning: Unfeasible length constraints: `min_length` (453) is larger than the maximum possible length (300). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ 4/4 (746 —Å–∏–º–≤–æ–ª–æ–≤)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:telegram.ext.Application:No error handlers are registered, logging exception.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/telegram/ext/_application.py\", line 1195, in process_update\n",
            "    await coroutine\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/telegram/ext/_basehandler.py\", line 153, in handle_update\n",
            "    return await self.callback(update, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-772064011.py\", line 1199, in button_click\n",
            "    await handle_show_original(update, context)\n",
            "  File \"/tmp/ipython-input-772064011.py\", line 1153, in handle_show_original\n",
            "    await query.message.reply_text(text, parse_mode='Markdown')\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/telegram/_message.py\", line 1095, in reply_text\n",
            "    return await self.get_bot().send_message(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/telegram/ext/_extbot.py\", line 2631, in send_message\n",
            "    return await super().send_message(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/telegram/_bot.py\", line 519, in decorator\n",
            "    result = await func(self, *args, **kwargs)  # skipcq: PYL-E1102\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/telegram/_bot.py\", line 840, in send_message\n",
            "    return await self._send_message(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/telegram/ext/_extbot.py\", line 529, in _send_message\n",
            "    result = await super()._send_message(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/telegram/_bot.py\", line 697, in _send_message\n",
            "    result = await self._post(\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/telegram/_bot.py\", line 607, in _post\n",
            "    return await self._do_post(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/telegram/ext/_extbot.py\", line 347, in _do_post\n",
            "    return await super()._do_post(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/telegram/_bot.py\", line 635, in _do_post\n",
            "    return await request.post(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/telegram/request/_baserequest.py\", line 168, in post\n",
            "    result = await self._request_wrapper(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/telegram/request/_baserequest.py\", line 325, in _request_wrapper\n",
            "    raise BadRequest(message)\n",
            "telegram.error.BadRequest: Can't parse entities: can't find end of the entity starting at byte offset 2836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–æ—Ç–∞...\n",
            "üîÑ –ó–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É –±–æ—Ç–∞...\n",
            "üõë –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        }
      ]
    }
  ]
}